{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Transaction Type (BUY or SELL) Via Stock Trades by Members of the US House of Representatives\n",
    "\n",
    "**James Bentley, Haicheng Xu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "\n",
    "### Introduction\n",
    "Objective: Predicting whether a trade is BUY or SELL.\n",
    "\n",
    "Type: Binary classification: there is only BUY (1) or SELL (0).\n",
    "\n",
    "Response Variable: type of transaction (BUY/SELL). We chose this as a indicator to detect market trends.\n",
    "\n",
    "Metric: F1-Score. We are using this because it a good balance between precision and recall. We believe precision and recall are important when predicting whether a trade is a buy or sell. \n",
    "\n",
    "### Baseline Model\n",
    "Made our first Pipeline using the KNNeighborsClassifier model\n",
    "\n",
    "    Features: \n",
    "        Quantitative:\n",
    "            1. Day of transaction \n",
    "            2. Month of transaction\n",
    "            3. Year of transaction        \n",
    "        Nominal:\n",
    "            4. Party\n",
    "        \n",
    "\n",
    "How: For the quantitative features, we used helper functions to strip respective parts of the date string. For 'party', we used one hot encoding to encode categorical values to numerical values.\n",
    "\n",
    "Why: For quantitative columns, buy or sell depends on the date since historical events such as the 2008 recession can cause systematic changes to buying and selling behaviors. For 'party', the buy or sell can depend on party affiliation since the parties could have different buying and selling patterns\n",
    "\n",
    "Model Performance: Using F1-Score for our metric of evaluation, we achieved a score of 0.6602746825602488 on our test set. We are able to conclude that this model is decent, since it's a lot better than the baseline accuracy of 0.5252973381159146 (guessing all 'buy'). Our pipeline improved our F-1 score by 0.13497734444433418.\n",
    "\n",
    "Generalization Ability: Our baseline model ability to generalize on unseen is mediocre since our pipeline only improved by around 13.5 percent. \n",
    "\n",
    "### Final Model\n",
    "\n",
    "Final Model\n",
    "\n",
    "    New Features:\n",
    "        Quantitative:\n",
    "            1. Day of disclosure \n",
    "            2. Month of disclosure\n",
    "            3. Year of disclosure\n",
    "            4. est_amount\n",
    "        Nominal:\n",
    "            5. ticker\n",
    "\n",
    "How: For the quantitative features (beside est_amount), we used helper functions to strip respective parts of the date string. For est_amount, we converted it to z-score with standard scaler. For 'ticker', we used one hot encoding to encode categorical values to numerical values.\n",
    "\n",
    "Why: For 'ticker', the buy or sell can depend on the performance of a specific stock. For example, when a company has a risk of being delisted, there is a higher proportion of individuals selling than buying that stock. For quantitative columns, buy or sell depends on disclosure date since congress members would likely disclose their buys and sales on separate dates. Therefore, we added disclosure dates in addition to transaction date.\n",
    "\n",
    "Generalization Ability: We improved the final model's F1-Score from 0.5252973381159146 to 0.7225288509784245, or about a 20 percent increase. With an F1-Score of 0.7225288509784245, the final model's ability to generalize on unseen data is a lot better than the baseline model. \n",
    "\n",
    "### Fairness Analysis\n",
    "\n",
    "Null Hypothesis: Our model is fair. Its F1-Score for Republicans and Democrats are roughly the same, and differences would be due to random chance.\n",
    "\n",
    "Alternative Hypothesis: Our model is unfair. Its F1-Score for Republicans and Democrats are significantly different.\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "p-value: 0.0\n",
    "\n",
    "Conclusion: We observed a p-value of 0.0, which is less than our alpha of 0.05. Therefore, we have enough evidence to reject our null hypothesis. Therefore, our model is unfairly predicting more accurately for Republicans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in cleaned stock transaction file that we combined with party affiliation from project 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('transactions_w_party.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped the index column\n",
    "df = df.drop(['index'], axis = 1)\n",
    "# Dropped the three Libertarian Party member's rows to simplify our model\n",
    "df = df[df['Party']!='Libertarian']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplified the four exchange categories, 'purchase', 'sale_partial, 'sale_full', 'exchange' to just buy and sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We dropped exchange\n",
    "df = df[df['type']!='exchange']\n",
    "df['type'] = df['type'].replace({'purchase': 'buy', 'sale_partial': 'sell', 'sale_full': 'sell'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created training and testing data for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = df[['transaction_date','Party']]\n",
    "y = df['type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to be used in our function transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns string date into number of days since year 0 for the transaction_date column\n",
    "def num_tdays(df):\n",
    "    return pd.DataFrame(df['transaction_date'].transform(lambda ser: int(ser.split('-')[0]) * 365 + int(ser.split('-')[1]) * 30 + int(ser.split('-')[-1])))\n",
    "# Turns string date into number of days since year 0 for the disclosure_date column\n",
    "def num_ddays(df):\n",
    "    return pd.DataFrame(df['disclosure_date'].transform(lambda ser: int(ser.split('-')[0]) * 365 + int(ser.split('-')[1]) * 30 + int(ser.split('-')[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns string date into day of the month for transaction_date\n",
    "def day(df):\n",
    "    return pd.DataFrame(df['transaction_date'].transform(lambda ser: int(ser.split('-')[-1])))\n",
    "# Turns string date into month for transaction_date\n",
    "def month(df):\n",
    "    return pd.DataFrame(df['transaction_date'].transform(lambda ser: int(ser.split('-')[1])))\n",
    "#Turns string date into year for transaction_date\n",
    "def year(df):\n",
    "    return pd.DataFrame(df['transaction_date'].transform(lambda ser: int(ser.split('-')[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made our first Pipeline using the KNNeighborsClassifier model\n",
    "\n",
    "    Features: \n",
    "        Quantitative:\n",
    "            1. Day of transaction \n",
    "            2. Month of transaction\n",
    "            3. Year of transaction        \n",
    "        Nominal:\n",
    "            4. Party\n",
    "How: For the quantitative features, we used helper functions to strip respective parts of the date string. For 'party', we used one hot encoding to encode categorical values to numerical values.\n",
    "\n",
    "Why: For quantitative columns, buy or sell depends on the date since historical events such as the 2008 recession can cause systematic changes to buying and selling behaviors. For 'party', the buy or sell can depend on party affiliation since the parties could have different buying and selling patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7021872265966754"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column Transformer to encode the four above features\n",
    "feature_eng_pipeline = ColumnTransformer([\n",
    "    ('day', FunctionTransformer(day), ['transaction_date']),\n",
    "    ('month', FunctionTransformer(month), ['transaction_date']),\n",
    "    ('year', FunctionTransformer(year), ['transaction_date']),\n",
    "    ('nominal', OneHotEncoder(), ['Party'])]\n",
    ")\n",
    "# Pipeline to make combine column transforming and KNN Classifier\n",
    "pl = Pipeline([\n",
    "    # Performs feature engineering \n",
    "    ('features', feature_eng_pipeline),\n",
    "    ('tree', KNeighborsClassifier(n_neighbors=3))\n",
    "])\n",
    "# Fits the training data\n",
    "pl.fit(X_train, y_train)\n",
    "# F1 Score for the training set\n",
    "f1_score(pl.predict(X_train), np.array(y_train),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363636363636364"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 Score for testing set\n",
    "f1_score(pl.predict(X_test), np.array(y_test),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5280347366433831"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline accuracy\n",
    "np.mean(y_train == 'buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13497734444433418"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# improvement \n",
    "0.6602746825602488 - 0.5252973381159146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance:\n",
    "    \n",
    "    Using F1-Score for our metric of evaluation, we achieved a score of 0.6602746825602488. We are able to conclude that this model is decent, since it's a lot better than the baseline accuracy of 0.5252973381159146 (guessing all 'buy'). Our pipeline improved our F-1 score by 0.13497734444433418."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Features: \n",
    "    \n",
    "    Quantitative:\n",
    "        1. Day of disclosure \n",
    "        2. Month of disclosure\n",
    "        3. Year of disclosure\n",
    "        4. est_amount\n",
    "    Nominal:\n",
    "        4. ticker\n",
    "        \n",
    "How: For the quantitative features (beside est_amount), we used helper functions to strip respective parts of the date string. For est_amount, we converted it to z-score with standard scaler. For 'ticker', we used one hot encoding to encode categorical values to numerical values.\n",
    "\n",
    "Why: For 'ticker', the buy or sell can depend on the performance of a specific stock. For example, when a company has a risk of being delisted, there is a higher proportion of individuals selling than buying that stock. For quantitative columns, buy or sell depends on disclosure date since congress members would likely disclose their buys and sales on separate dates. Therefore, we added disclosure dates in addition to transaction date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X = df[['transaction_date','est_amount','Party','disclosure_date','ticker']]\n",
    "y = df['type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns string date into day of the month for disclosure_date\n",
    "def dday(df):\n",
    "    return pd.DataFrame(df['disclosure_date'].transform(lambda ser: ser.split('-')[-1]))\n",
    "# Turns string date into month for disclosure_date\n",
    "def dmonth(df):\n",
    "    return pd.DataFrame(df['disclosure_date'].transform(lambda ser: ser.split('-')[1]))\n",
    "#Turns string date into year for disclosure_date\n",
    "def dyear(df):\n",
    "    return pd.DataFrame(df['disclosure_date'].transform(lambda ser: ser.split('-')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8575771122038445"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN ClASSIFIER WITH NEW FEATURES\n",
    "\n",
    "feature_eng_pipeline = ColumnTransformer([\n",
    "        ('dday', FunctionTransformer(dday), ['disclosure_date']),\n",
    "        ('dmonth', FunctionTransformer(dmonth), ['disclosure_date']),\n",
    "        ('dyear', FunctionTransformer(dyear), ['disclosure_date']),\n",
    "        ('tday', FunctionTransformer(day), ['transaction_date']),\n",
    "        ('month', FunctionTransformer(month), ['transaction_date']),\n",
    "        ('year', FunctionTransformer(year), ['transaction_date']),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'), ['Party','ticker']),\n",
    "        ('quant', StandardScaler(), ['est_amount'])\n",
    "])\n",
    "pl = Pipeline([\n",
    "    ('features', feature_eng_pipeline),\n",
    "    ('tree', KNeighborsClassifier(n_neighbors=3))\n",
    "])\n",
    "pl.fit(X_train, y_train)\n",
    "f1_score(pl.predict(X_train), np.array(y_train),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69800796812749"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pl.predict(X_test), np.array(y_test),pos_label='buy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Model Selection:\n",
    "    We manually checked two other models: DecisionTreeClassifier, and RandomTreeClassifier. Using the same features in each model, we found that our F1-score was the highest using the DecisionTreeClassifier, with KNeighborsClassifier in second, and RandomTreeClassifier in a close third. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7848828185627813"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RANDOM FOREST CLASSIFIER\n",
    "\n",
    "feature_eng_pipeline = ColumnTransformer([\n",
    "        ('dday', FunctionTransformer(dday), ['disclosure_date']),\n",
    "        ('dmonth', FunctionTransformer(dmonth), ['disclosure_date']),\n",
    "        ('dyear', FunctionTransformer(dyear), ['disclosure_date']),\n",
    "        ('tday', FunctionTransformer(day), ['transaction_date']),\n",
    "        ('month', FunctionTransformer(month), ['transaction_date']),\n",
    "        ('year', FunctionTransformer(year), ['transaction_date']),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'), ['Party','ticker']),\n",
    "        ('quant', StandardScaler(), ['est_amount'])\n",
    "])\n",
    "pl = Pipeline([\n",
    "    ('features', feature_eng_pipeline),\n",
    "    ('tree', RandomForestClassifier(max_depth=19))\n",
    "])\n",
    "pl.fit(X_train, y_train)\n",
    "f1_score(pl.predict(X_train), np.array(y_train),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6988399071925754"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pl.predict(X_test), np.array(y_test),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8855039350088855"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DECISION TREE CLASSIFIER\n",
    "\n",
    "feature_eng_pipeline = ColumnTransformer([\n",
    "        ('dday', FunctionTransformer(dday), ['disclosure_date']),\n",
    "        ('dmonth', FunctionTransformer(dmonth), ['disclosure_date']),\n",
    "        ('dyear', FunctionTransformer(dyear), ['disclosure_date']),\n",
    "        ('tday', FunctionTransformer(day), ['transaction_date']),\n",
    "        ('month', FunctionTransformer(month), ['transaction_date']),\n",
    "        ('year', FunctionTransformer(year), ['transaction_date']),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'), ['Party','ticker']),\n",
    "        ('quant', StandardScaler(), ['est_amount'])\n",
    "])\n",
    "pl = Pipeline([\n",
    "    ('features', feature_eng_pipeline),\n",
    "    ('tree', DecisionTreeClassifier(max_depth=19))\n",
    "])\n",
    "pl.fit(X_train, y_train)\n",
    "f1_score(pl.predict(X_train), np.array(y_train),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7225288509784245"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pl.predict(X_test), np.array(y_test),pos_label='buy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### GridSearch:\n",
    "    Using GridSearchCV, we found that the best max_depth hyperparameter is 19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 ColumnTransformer(transformers=[('num_dday',\n",
       "                                                  FunctionTransformer(func=<function num_ddays at 0x000002990C4F2D38>),\n",
       "                                                  ['disclosure_date']),\n",
       "                                                 ('dmonth',\n",
       "                                                  FunctionTransformer(func=<function dmonth at 0x000002990C44B0D8>),\n",
       "                                                  ['disclosure_date']),\n",
       "                                                 ('dyear',\n",
       "                                                  FunctionTransformer(func=<function dyear at 0x000002990C44B558>),\n",
       "                                                  ['disclosure_date']),\n",
       "                                                 ('day',\n",
       "                                                  FunctionTransformer(func=<function day at 0x000002990C1051F8>),\n",
       "                                                  ['transaction_date']),\n",
       "                                                 ('month',\n",
       "                                                  FunctionTransformer(func=<function month at 0x000002990C105168>),\n",
       "                                                  ['transaction_date']),\n",
       "                                                 ('year',\n",
       "                                                  FunctionTransformer(func=<function year at 0x000002990C1055E8>),\n",
       "                                                  ['transaction_date']),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
       "                                                  ['Party', 'ticker'])])),\n",
       "                ('tree', DecisionTreeClassifier(max_depth=19))])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = {'tree__max_depth': np.arange(1,20)}\n",
    "searcher = GridSearchCV(pl, hyperparameters)\n",
    "searcher.fit(X_train, y_train)\n",
    "searcher.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Model\n",
    "    Classification Model: Decision Tree Classifier\n",
    "    Parameter: Disclosure date, transaction date, party, and ticker\n",
    "    Hyperparameter: max_depth = 19\n",
    "    F1-Score: 0.7225288509784245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8855039350088855"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FINAL MODEL\n",
    "\n",
    "feature_eng_pipeline = ColumnTransformer([\n",
    "        ('dday', FunctionTransformer(dday), ['disclosure_date']),\n",
    "        ('dmonth', FunctionTransformer(dmonth), ['disclosure_date']),\n",
    "        ('dyear', FunctionTransformer(dyear), ['disclosure_date']),\n",
    "        ('tday', FunctionTransformer(day), ['transaction_date']),\n",
    "        ('month', FunctionTransformer(month), ['transaction_date']),\n",
    "        ('year', FunctionTransformer(year), ['transaction_date']),\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'), ['Party','ticker']),\n",
    "        ('quant', StandardScaler(), ['est_amount'])\n",
    "])\n",
    "pl = Pipeline([\n",
    "    ('features', feature_eng_pipeline),\n",
    "    ('tree', DecisionTreeClassifier(max_depth=19))\n",
    "])\n",
    "pl.fit(X_train, y_train)\n",
    "f1_score(pl.predict(X_train), np.array(y_train),pos_label='buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7225288509784245"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Model F1 Score\n",
    "f1_score(pl.predict(X_test), np.array(y_test),pos_label='buy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis: Our model is fair. Its F1-Score for Republicans and Democrats are roughly the same, and differences would be due to random chance.\n",
    "\n",
    "Alternative Hypothesis: Our model is unfair. Its F1-Score for Republicans and Democrats are significantly different.\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "p-value: 0.0\n",
    "\n",
    "Conclusion: We observed a p-value of 0.0, which is less than our alpha of 0.05. Therefore, we have enough evidence to reject our null hypothesis. Therefore, our model is unfairly predicting more accurately for Republicans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing observed test statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "Republican = df[df['Party'] == 'Republican']\n",
    "Democratic = df[df['Party'] == 'Democratic']\n",
    "\n",
    "# train test split REPUBLICAN\n",
    "\n",
    "Xr = Republican[['transaction_date','est_amount','Party','disclosure_date','ticker']]\n",
    "yr = Republican['type']\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.25)\n",
    "pl.fit(Xr_train, yr_train)\n",
    "\n",
    "r_f1 = f1_score(pl.predict(Xr_test), np.array(yr_test),pos_label='buy')\n",
    "\n",
    "# train test split DEMOCRATIC\n",
    "\n",
    "Xd = Democratic[['transaction_date','est_amount','Party','disclosure_date','ticker']]\n",
    "yd = Democratic['type']\n",
    "Xd_train, Xd_test, yd_train, yd_test = train_test_split(Xd, yd, test_size=0.25)\n",
    "pl.fit(Xd_train, yd_train)\n",
    "\n",
    "d_f1 = f1_score(pl.predict(Xd_test), np.array(yd_test),pos_label='buy')\n",
    "\n",
    "# observed test statistics\n",
    "observed_abs_diff = np.abs(r_f1-d_f1)\n",
    "observed_diff = r_f1-d_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1488955689445054"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_abs_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation\n",
    "results = []\n",
    "df_p = df.copy()\n",
    "\n",
    "for _ in range(200):\n",
    "    \n",
    "    # Permutation\n",
    "    \n",
    "    df_p['Shuffled_Party'] = np.random.permutation(df_p['Party'])\n",
    "    Republican = df_p[df_p['Shuffled_Party'] == 'Republican']\n",
    "    Democratic = df_p[df_p['Shuffled_Party'] == 'Democratic']\n",
    "    \n",
    "    # Train test split Republican\n",
    "    \n",
    "    Xr = Republican[['transaction_date','est_amount','Party','disclosure_date','ticker']]\n",
    "    yr = Republican['type']\n",
    "    Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr, yr, test_size=0.25)\n",
    "    pl.fit(Xr_train, yr_train)\n",
    "    # Republican F1-Score\n",
    "    r_f1 = f1_score(pl.predict(Xr_test), np.array(yr_test),pos_label='buy')\n",
    "\n",
    "\n",
    "    # Train test split Democratic\n",
    "    \n",
    "    Xd = Democratic[['transaction_date','est_amount','Party','disclosure_date','ticker']]\n",
    "    yd = Democratic['type']\n",
    "    Xd_train, Xd_test, yd_train, yd_test = train_test_split(Xd, yd, test_size=0.25)\n",
    "    pl.fit(Xd_train, yd_train)\n",
    "    # Democratic F1-Score\n",
    "    d_f1 = f1_score(pl.predict(Xd_test), np.array(yd_test),pos_label='buy')\n",
    "\n",
    "    \n",
    "    abs_diff = np.abs(r_f1-d_f1)\n",
    "    \n",
    "    # Test statistics\n",
    "    results.append(abs_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = np.mean(results >= observed_abs_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c748a23e82a04e3aeddf1df0cbe51bfac9883ec65687ad42593cbf64c7ecd29"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
